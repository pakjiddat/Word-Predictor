#' It is used to perform extrinsic and intrinsic evaluation of a model.
#'
#' @description
#' It provides methods for performing extrinsic and intrinsic
#' evaluation. Intrinsic evaluation is based on calculation of Perplexity.
#' Extrinsic evaluation is based on accuracy. It involves determining the
#' percentage of correct next word predictions.
#'
#' @details
#' Before performing the intrinsic and extrinsic model evaluation, a
#' validation file must be first generated. This can be done using the Generator
#' class. Each line in the validation file is evaluated. For intrinsic
#' evaluation Perplexity for the line is calculated. An overall summary of the
#' Perplexity calculations is returned. It includes the min, max and mean
#' Perplexity. For extrinsic evaluation, next word prediction is performed on
#' each line. If the actual next word is one of the three predicted next words,
#' then the prediction is considered to be accurate. The extrinsic evaluation
#' returns the percentage of correct and incorrect predictions.
ModelEvaluator <- R6::R6Class(
    "ModelEvaluator",
    inherit = TextFileProcessor,
    public = list(
        #' @description
        #' It initializes the current object. It is used to set the
        #' model file name and verbose options.
        #' @param mf The model file name.
        #' @param ve If progress information should be displayed.
        #' @export
        initialize = function(mf, ve = 0) {
            # The base class is initialized
            super$initialize(NULL, NULL, verbose)
            # If the model file name is not valid, then an error is thrown
            if (!file.exists(mf))
                stop(paste0("Invalid model file: ", model_file))
            else {
                # The model file name is set
                private$mf <- mf
                # The model object is read
                private$m <- private$read_obj(model_file)
            }
        },

        #' @description
        #' It performs intrinsic and extrinsic evaluation for the
        #' given model. It also measures the memory usage and time taken. The
        #' performance metrics are displayed in 5 plots on one page. Performance
        #' statistics are saved to the model object.
        #' @param lc The number of lines of text in the validation file to be
        #'   used for the evaluation.
        #' @param fn The name of the validation file. If it does not exist, then
        #'   the default file validation-clean.txt is checked in the models
        #'   folder
        performance_evaluation = function(lc, fn) {
            # The y-axis values
            pstats <- list("m" = NULL, "t" = NULL, "p" = NULL, "a" = NULL)
            time_taken <- system.time({
                memory_used <- mem_change({
                    # Intrinsic evaluation is performed
                    istats <- self$intrinsic_evaluation(lc, fn)
                    # Extrinsic evaluation is performed
                    estats <- self$extrinsic_evaluation(lc, fn)
                })
            })
            # The memory used
            memory_used <- object_size(private$model)
            # The y-axis values are updated
            pstats[["m"]] <- private$format_size(memory_used)
            pstats[["t"]] <- time_taken[[3]]
            pstats[["p"]] <- istats$mean
            pstats[["a"]] <- estats$valid_perc
            # The performance stats are saved
            private$m$pstats <- pstats
            # The model is saved
            private$save_obj(private$m, private$mf)
        },

        #' @description
        #' It compares the performance of the specified models by
        #' plotting the performance statistics generated by the
        #' performance_evaluation function. The models are specified with the
        #' type parameter.
        #' @param type The models to compare. It can be: 'basic' -> The models
        #'   for each n-gram size are compared. 'grouped' -> Each model folder
        #'   contains n-gram models for a given input data size. For e.g For
        #'   data size 5 Mb, there are 3 models for the ngrams 2:4. The
        #'   performance of all models in all folders is plotted on a graph.
        #' @param opts A list of options for plotting the data. 'group' -> The
        #'   field to group by. 'title' -> The main plot title. 'subtitle' ->
        #'   The plot sub title.
        #' @return The performance stats.
        performance_comparision = function(type, opts) {
            # If the type is 'basic'
            if (type == 'basic') {
                # The pstats file name
                fn <- paste0(private$mdir, "/pstats.RDS")
                # The stats are read
                pstats <- private$read_obj(fn)
                # The config file name
                fn <- paste0(private$mdir, "/config.RDS")
                # The config file is read
                config <- private$read_obj(fn)
                # The y-axis values
                y <- list("m" = NULL, "t" = NULL, "p" = NULL, "a" = NULL)
                # For each model the performance metrics are measured
                for (i in 1:(config$model-1)) {
                    # The memory value
                    m <- private$format_size(pstats[[i]][["memory"]])
                    # The y-axis values are updated
                    y[["m"]] <- c(y[["m"]],  m)
                    y[["t"]] <- c(y[["t"]], pstats[[i]][["time"]])
                    y[["p"]] <- c(y[["p"]],
                                  pstats[[i]][["intrinsic"]][["mean"]])
                    y[["a"]] <- c(y[["a"]],
                                  pstats[[i]][["extrinsic"]][["valid_perc"]])
                }
                # The data frame containing the data to be plotted
                df <- data.frame(
                    "n" = 2:(length(pstats) + 1),
                    "m" = y[["m"]],
                    "t" = y[["t"]],
                    "p" = y[["p"]],
                    "a" = y[["a"]]
                )
                # The options for plotting
                opts <- list(
                    "type" = "basic",
                    "title" = "Variation of performance with ngram size",
                    "subtitle" = opts[['subtitle']])
            }
            # If the type is 'grouped'
            else if (type == 'grouped') {
                # The average performance stats for each data size
                y <- list("m" = NULL, "t" = NULL, "p" = NULL, "a" = NULL)
                # The different items in the group. Each folder is a group item
                f <- list.dirs(private$mdir, recursive = F, full.names = F)
                # The output data frame
                df <- data.frame()
                # For each folder, the performance stats for all ngram models is
                # calculated.
                for (i in f) {
                    # The performance stats file name
                    fn <- paste0(private$mdir, "/", i, "/pstats.RDS")
                    # The stats are read
                    pstats <- private$read_obj(fn)
                    # The config file name
                    fn <- paste0(private$mdir, "/", i, "/config.RDS")
                    # The stats are read
                    config <- private$read_obj(fn)
                    # The memory usage for each ngram
                    m <- private$process_stats(pstats, "memory", config$model)
                    # The time taken for each ngram
                    t <- private$process_stats(pstats, "time", config$model)
                    # The perplexity for each ngram
                    p <- private$process_stats(pstats, "perplexity", config$model)
                    # The accuracy score for each ngram
                    a <- private$process_stats(pstats, "accuracy", config$model)
                    # The temprary data frame for the current data size
                    tdf <- data.frame(
                        rep(i, (config$model-1)),
                        m, t, p, a, 2:(length(pstats) + 1))
                    # The column names are set
                    names(tdf) <- c(opts[["group"]], "m", "t", "p", "a", "n")
                    # The temprary data frame is appended to the output data
                    # frame
                    df <- rbind(df, tdf)
                }
                # The group name
                g <- opts[["group"]]
                # The options for plotting
                opts <- list(
                    "type" = "grouped",
                    "group" = g,
                    "title" = opts[['title']],
                    "subtitle" = opts[['subtitle']])
                # If the group column is numeric
                if (is.numeric(as.numeric(as.character(df[[g]])))) {
                    # The group column is converted to numeric
                    df[[g]] <- as.numeric(as.character(df[[g]]))
                    # The data is ordered by group
                    df <- df[order(df[[opts[["group"]]]]), ]
                    # The group column is converted to factor
                    df[[g]] <- as.factor(df[[g]])
                }
            }
            # The performance stats are plotted
            private$plot_stats(df, opts)
            # The performance stats are returned
            return(df)
        },

        #' @description
        #' Evaluates the model using intrinsic evaluation based on
        #' Perplexity. The given number of sentences are taken from the
        #' validation file. For each sentence, the Perplexity is calculated.
        #' @param lc The number of lines of text in the validation file to be
        #'   used for the evaluation.
        #' @param fn The name of the validation file. If it does not exist, then
        #'   the default file validation-clean.txt is checked in the models
        #'   folder
        #' @return The min, max and mean Perplexity score.
        intrinsic_evaluation = function(lc, fn) {
            # The Predictor class object is created
            pr <- Predictor$new()
            # The validation data is read
            data <- private$read_validation_data(lc, "I", fn)
            # The list of perplexities
            pl <- c();
            # The loop counter
            c <- 1
            # The Perplexity of each sentence in the test data is calculated
            for (line in data) {
                # The line is split on space
                words <- str_split(line, " ")[[1]]
                # The perplexity for the line is calculated
                p <- pr$calc_perplexity(words)
                # The information message
                msg <- paste0(
                    "Perplexity of the sentence '",
                    line, "' is: ", p)
                # The information message is shown
                private$display_msg(msg, 2)
                # The list of perplexities is updated
                pl <- c(pl, p);
                # If the counter is divisible by 10
                if (c %% 10 == 0) {
                    # The information message
                    msg <- paste0(c, " lines have been processed")
                    # The information message is shown
                    private$display_msg(msg, 1)
                }
                # The counter is increased by 1
                c <- c + 1
            }
            # The perplexity stats
            stats <- list(
                "min" = min(pl),
                "max" = max(pl),
                "mean" = mean(pl));

            return(stats);
        },

        #' @description
        #' Evaluates the model using extrinsic evaluation based on
        #' Accuracy. The given number of sentences are taken from the validation
        #' file. For each sentence, the model is used to predict the next word.
        #' The accuracy stats are returned. A prediction is considered to be
        #' correct if one of the predicted words matches the actual word.
        #' @param lc The number of lines of text in the validation file to be
        #'   used for the evaluation.
        #' @param fn The name of the validation file. If it does not exist, then
        #'   the default file validation-clean.txt is checked in the models
        #'   folder
        #' @return The number of correct and incorrect predictions.
        extrinsic_evaluation = function(lc, fn) {
            # The Predictor class object is created
            pr <- Predictor$new()
            # The validation data is read
            data <- private$read_validation_data(lc, "E", fn)
            # The statistics
            stats <- list("valid" = 0, "invalid" = 0)
            # The loop counter
            c <- 1
            # The last word for each sentence is predicted
            for (line in data) {
                # The line is split on space
                words <- str_split(line, " ")[[1]]
                # The word to predict
                w <- words[length(words)]
                # The previous words used to predict the word
                pw <- words[1:length(words)-1]
                # If the words should be stemmed
                if (private$tg_opts[["stem_words"]]) {
                    # The previous words are stemmed
                    pw <- wordStem(pw)
                }
                # The next word is predicted
                res <- pr$predict_word(pw, F)
                # If the predicted word matches the actual word
                if (w %in% res["words"]) {
                    stats[["valid"]] <- stats[["valid"]] + 1;
                    # The information message
                    private$display_msg(
                        paste0("The word: ", w, " was predicted"), 3)
                }
                # If the predicted word does not match
                else {
                    stats[["invalid"]] <- stats[["invalid"]] + 1;
                    # The information message
                    private$display_msg(
                        paste0("The word: ", w, " could not be predicted"), 3)
                }
                # The counter is increased by 1
                c <- c + 1
                # If the counter is divisible by 10
                if (c %% 10 == 0) {
                    # The information message
                    msg <- paste0(c, " sentences have been processed")
                    # The information message is shown
                    private$display_msg(msg, 1)
                }
            }

            # The precentage of valid
            stats[["valid_perc"]] <-
                (stats[["valid"]]/(stats[["valid"]] + stats[["invalid"]]))*100
            # The precentage of invalid
            stats[["invalid_perc"]] <- 100-stats[["valid_perc"]]

            return(stats)
        }
    ),

    private = list(
        # @field m The model object.
        m = NULL,
        # @field mf The path to the model file.
        mf = NULL
    )
)
